{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Framework Tutorial\n",
    "\n",
    "This guide demonstrates how to use the custom neural network framework for various machine learning tasks.\n",
    "\n",
    "## Contents:\n",
    "1. Loading and Preparing Data\n",
    "2. Creating Neural Network Architecture\n",
    "3. Training Process\n",
    "4. Examples:\n",
    "   - Classification (Iris Dataset)\n",
    "   - Regression (Diabetes Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Preparing Data\n",
    "\n",
    "First, let's import necessary modules and explore the DataLoader class capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: ['iris', 'mnist', 'diabetes', 'wine']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/cube/Documents/Development/MAI/Сошников\")\n",
    "from framework.DataLoader import DataLoader\n",
    "\n",
    "# Create DataLoader instance\n",
    "dl = DataLoader()\n",
    "\n",
    "# List available datasets\n",
    "print(\"Available datasets:\", dl.list_datasets())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader Features:\n",
    "- Loading different datasets\n",
    "- Data preprocessing\n",
    "- Batch creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "data_shape: (150, 4)\n",
      "labels_shape: (150,)\n",
      "data_type: float64\n",
      "labels_type: int64\n",
      "unique_labels: [0 1 2]\n",
      "data_mean: 3.4644999999999997\n",
      "data_std: 1.9738430577598278\n",
      "\n",
      "First 3 samples:\n",
      "Data: [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]]\n",
      "Labels: [0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Load Iris dataset\n",
    "dl = DataLoader(\"iris\")\n",
    "\n",
    "# Get dataset statistics\n",
    "stats = dl.get_stats()\n",
    "print(\"\\nDataset Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# View first few samples\n",
    "data, labels = dl.head(3)\n",
    "print(\"\\nFirst 3 samples:\")\n",
    "print(\"Data:\", data)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network Architecture\n",
    "\n",
    "Our framework provides various components for building neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Activation Functions:\n",
      "- ReLU\n",
      "- Sigmoid\n",
      "- Tanh\n",
      "- LeakyReLU\n",
      "\n",
      "Available Optimizers:\n",
      "- SGD\n",
      "- MomentumSGD\n",
      "- RMSprop\n",
      "- Adam\n"
     ]
    }
   ],
   "source": [
    "from framework.NeuralNetwork import NeuralNetwork, Layer, ActivationFunction, Optimizer\n",
    "\n",
    "# Available Activation Functions:\n",
    "print(\"Available Activation Functions:\")\n",
    "print(\"- ReLU\")\n",
    "print(\"- Sigmoid\")\n",
    "print(\"- Tanh\")\n",
    "print(\"- LeakyReLU\")\n",
    "\n",
    "# Available Optimizers:\n",
    "print(\"\\nAvailable Optimizers:\")\n",
    "print(\"- SGD\")\n",
    "print(\"- MomentumSGD\")\n",
    "print(\"- RMSprop\")\n",
    "print(\"- Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Neural Network\n",
    "\n",
    "Let's create a simple neural network for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network parameters\n",
    "input_size = 4  # Iris dataset has 4 features\n",
    "hidden_size = 10\n",
    "output_size = 3  # Iris has 3 classes\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer.Adam(learning_rate=0.01)\n",
    "\n",
    "# Create neural network\n",
    "model = NeuralNetwork(optimizer)\n",
    "\n",
    "# Add layers\n",
    "model.add_layer(Layer(input_size, hidden_size, ActivationFunction.ReLU))\n",
    "model.add_layer(Layer(hidden_size, output_size, ActivationFunction.Sigmoid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Process\n",
    "\n",
    "Here's how to train your neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 1.0384\n",
      "Epoch 20/50, Loss: 0.6267\n",
      "Epoch 30/50, Loss: 0.5410\n",
      "Epoch 40/50, Loss: 0.4422\n",
      "Epoch 50/50, Loss: 0.3420\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare data\n",
    "dl.shuffle()\n",
    "X_train, X_test, y_train, y_test = train_test_split(dl.data, dl.labels, test_size=0.2)\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    batches = dl.batch(batch_size)\n",
    "    \n",
    "    for X_batch, y_batch in batches:\n",
    "        # Forward pass\n",
    "        y_pred = model.forward(X_batch)\n",
    "        \n",
    "        # Convert labels to one-hot encoding\n",
    "        y_one_hot = np.zeros((y_batch.size, output_size))\n",
    "        y_one_hot[np.arange(y_batch.size), y_batch.astype(int)] = 1\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = -np.sum(y_one_hot * np.log(y_pred + 1e-10)) / y_batch.size\n",
    "        \n",
    "        # Backward pass\n",
    "        grad = (y_pred - y_one_hot) / y_batch.size\n",
    "        model.backward(grad)\n",
    "        \n",
    "        # Update weights\n",
    "        model.update()\n",
    "        \n",
    "        epoch_loss += loss\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss/len(batches):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Usage Tips\n",
    "\n",
    "### Choosing Activation Functions\n",
    "- ReLU: Good default choice for hidden layers\n",
    "- Sigmoid: Use for binary classification output\n",
    "- Tanh: Alternative to ReLU, works well with normalized data\n",
    "- LeakyReLU: Helps prevent \"dying ReLU\" problem\n",
    "\n",
    "### Optimizer Selection\n",
    "- SGD: Simple but may converge slowly\n",
    "- MomentumSGD: Better convergence than SGD\n",
    "- RMSprop: Good for non-stationary objectives\n",
    "- Adam: Best general-purpose optimizer\n",
    "\n",
    "### Architecture Tips\n",
    "1. Layer Sizing:\n",
    "   - Input layer: Match feature dimension\n",
    "   - Hidden layers: Usually powers of 2\n",
    "   - Output layer: Match number of classes (classification) or 1 (regression)\n",
    "\n",
    "2. Learning Rate:\n",
    "   - Start with 0.01 for Adam\n",
    "   - Start with 0.1 for SGD\n",
    "   - Decrease if training is unstable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
